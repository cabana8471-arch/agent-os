# Concept: Best Practices

Lessons learned from using Agent OS. Patterns that work, anti-patterns that don't.

---

## ğŸ“‹ Development Workflow Best Practices

### âœ… DO

- âœ… **Start with Planning**: `/agent-os:plan-product` first, always
- âœ… **Write Specs**: Spec before code, every time
- âœ… **Iterative Implementation**: Per feature, not whole app
- âœ… **Review Code**: Even automated reviews are valuable
- âœ… **Test Early**: Unit tests, not just E2E
- âœ… **Commit Frequently**: Logical chunks, not big blobs
- âœ… **Document as You Go**: README, API docs, comments
- âœ… **Refactor Incrementally**: Small improvements, not rewrites

### âŒ DON'T

- âŒ **Skip Planning**: "Let's just code!" = chaos
- âŒ **Feature Without Spec**: Rework, wasted effort
- âŒ **Big Bang Implementations**: 50 tasks in 1 go = slow + risky
- âŒ **Ignore Code Review**: These are real issues
- âŒ **Test Last**: Should test during dev
- âŒ **Giant Commits**: Hard to review + revert
- âŒ **No Documentation**: Next dev suffers
- âŒ **Big Rewrites**: High risk, high cost

---

## ğŸ”„ Commit Strategy

### Good Commit Pattern

```
1 feature = 1-3 commits

Example: Task CRUD API
- Commit 1: Database schema + models
- Commit 2: API endpoints
- Commit 3: Tests + documentation
```

**Benefits**:
- Easy to review (per commit)
- Easy to revert (if needed)
- Clear history

---

### Bad Commit Pattern

```
1 feature = 1 giant commit (500+ lines changed)
```

**Problems**:
- Hard to review
- Hard to revert
- Lost context

---

## ğŸš€ Phasing Strategy

### MVP First

```
Phase 1: Core features only (2-3 features)
  â””â”€ Deploy, validate with users

Phase 2: Growth features (2-3 more)
  â””â”€ Deploy, expand user base

Phase 3: Scale features (automation, perf)
  â””â”€ Deploy, scale infrastructure
```

**Benefits**: Get to market fast, learn from users

---

### NOT All-At-Once

```
âŒ Launch with 10 features at once
   Risk: Bugs in everything, no focus
```

---

## ğŸ” Testing Strategy

### Test Pyramid

```
         E2E Tests (10%)
       /              \
      Integration     Tests (30%)
     /                    \
   Unit Tests (60%)
```

**Recommended Ratio**:
- 60% Unit tests (fast, isolated)
- 30% Integration tests (real dependencies)
- 10% E2E tests (user workflows)

---

### When to Test

- âœ… **During dev**: Unit test as you code
- âœ… **Before merging**: Integration + E2E
- âœ… **Before deploy**: Full test suite
- âŒ **After deploy**: Too late for testing

---

## ğŸ”§ Error Handling Best Practices

### âœ… Good Error Handling

```
try {
  await createTask(data)
} catch (error) {
  if (error.code === 'VALIDATION_ERROR') {
    showUserMessage('Title required')
  } else if (error.code === 'AUTH_ERROR') {
    redirectToLogin()
  } else {
    logError(error)  // Log unexpected
    showGenericError('Something went wrong')
  }
}
```

**Benefits**: User-friendly, actionable

---

### âŒ Bad Error Handling

```
// Silent failure
try {
  await createTask(data)
} catch (error) {
  // Do nothing
}

// Generic message
catch (error) {
  showMessage('Error')  // Unhelpful
}
```

---

## ğŸ“Š Code Review Best Practices

### When Reviewing Code

- âœ… **Read thoroughly**: Don't skim
- âœ… **Run tests**: Don't assume they pass
- âœ… **Test manually**: Try edge cases
- âœ… **Check security**: SQL injection? XSS? Auth?
- âœ… **Ask questions**: "Why this approach?"
- âœ… **Suggest improvements**: Not demands
- âœ… **Approve when satisfied**: Don't block unnecessarily

### Code Review Checklist

```
Code Review Checklist:

Functionality
â–¡ Feature works as spec
â–¡ Edge cases handled
â–¡ Error handling complete

Quality
â–¡ Code readable (good names)
â–¡ No duplicates (DRY)
â–¡ Performance OK

Security
â–¡ Input validated
â–¡ No SQL injection risk
â–¡ No XSS risk
â–¡ Secrets not exposed

Standards
â–¡ Follows conventions
â–¡ Consistent style
â–¡ Tests included
```

---

## ğŸš¨ When to Refactor

### Good Refactoring Moments

- âœ… **After feature done**: While fresh in mind
- âœ… **Code duplication**: Extract to function
- âœ… **Complex function**: Split into smaller
- âœ… **Poor naming**: Rename for clarity
- âœ… **Technical debt**: Scheduled maintenance

### Bad Refactoring Moments

- âŒ **During feature dev**: Gets sidetracked
- âŒ **Before deploy**: Risky, untested
- âŒ **"While I'm at it"**: Scope creep
- âŒ **For style**: Minor nitpicks

---

## ğŸ¤ Team Coordination (Multi-Agent)

### When Using Multi-Agent

- âœ… **Clear task boundaries**: Agents don't step on each other
- âœ… **Defined APIs**: Between components
- âœ… **Regular syncs**: Check progress daily
- âœ… **Merge strategy**: Plan how to integrate

### Potential Issues

- âš ï¸ **Merge conflicts**: If same file edited
- âš ï¸ **Coordination overhead**: Need communication
- âš ï¸ **Integration issues**: Features not compatible

### Prevention

```
Before multi-agent:
1. Define clear task boundaries
2. Design interfaces/APIs between tasks
3. Plan merge points
4. Setup merge conflict resolution

During multi-agent:
1. Daily standup (sync progress)
2. Flag blockers early
3. Coordinate shared components
```

---

## ğŸ“ˆ Performance Optimization

### When to Optimize

- âœ… **After profile**: Know where slow is
- âœ… **User-facing**: UI responsiveness matters
- âœ… **Data-heavy**: Large lists, big datasets
- âœ… **Production**: Monitor real usage

### NOT

- âŒ **Premature optimization**: "It might be slow"
- âŒ **Guessing**: Profile first, then fix
- âŒ **Sacrificing readability**: Unreadable fast code is bad

---

## ğŸ“ Documentation Best Practices

### Document What

- âœ… **APIs**: Every endpoint documented
- âœ… **Architecture**: How components work together
- âœ… **Setup**: How to dev locally
- âœ… **Why**: Decisions, trade-offs, constraints
- âœ… **Examples**: Usage examples in README

### Not

- âŒ **Code comments**: "Good code is self-documenting"
- âŒ **Obvious stuff**: Variable name says what it is
- âŒ **Outdated docs**: Stale documentation = worse than none

---

## â° Timeline Estimation

### Realistic Estimates

```
Feature estimate = implementation + review + testing + docs

Task CRUD API (12 tasks, 50 SP):
- Estimate per task: 1 day
- Buffer (unknowns): +20% = 1.2 days/task
- TOTAL: 12 Ã— 1.2 = 14.4 days

Conservative: Add 1-2 more days buffer
Aggressive: Remove buffer (risky)
```

### NOT

```
"We can do this in 2 days"  (when it's really 2 weeks)
"Just small tweaks" (takes 3x longer)
```

---

## ğŸ”„ Iteration Strategy

### Iterate On

- âœ… **Spec**: If ambiguous or wrong
- âœ… **Implementation**: If taking too long
- âœ… **Architecture**: If design wrong
- âœ… **Process**: If workflow slow

### How

```
1. Assess: Is this taking too long? Why?
2. Pivot: Change spec, redesign, or approach
3. Try again: Re-implement with new approach
4. Learn: Document what worked better
```

---

## ğŸ“ Checklist: Before You Deploy

```
â–¡ All tests passing
â–¡ Code review approved
â–¡ No CRITICAL/HIGH issues in code review
â–¡ Spec met (verify spec)
â–¡ Documentation updated
â–¡ Performance acceptable
â–¡ Security audit passed
â–¡ User testing OK (if external)
â–¡ Monitoring setup
â–¡ Rollback plan ready
â–¡ Team aligned
```

---

## ğŸ’¡ Final Tips

1. **Start small**: MVP with 3 features, not 10
2. **Get feedback early**: Users know best
3. **Test often**: Catch issues fast
4. **Refactor gradually**: Pay off technical debt
5. **Document wisely**: Essential docs, not everything
6. **Optimize after**: Profile first, then optimize
7. **Communicate**: Team alignment prevents rework
8. **Iterate**: Perfect is enemy of shipped

---

**Remember: Done is better than perfect, but done well is best!** ğŸš€
